## 基本描述

* hashMap 区别于 hashTable/concurrentHashMap

  > 后者线程安全，前者反之；
  >
  > 后者k、v都不能为空，前者反之；前者多个空k视为同一个处理；

## 数据结构

* 底层容器是一个node数组

  > 数组初始默认长度16，达到指定条件后扩容一次(乘2)

* 扩容条件

  - 初始化扩容

    > map对象创建时并不创建容器(以map参数创建时除外)，第一次put时通过初始化创建
    >
    > 初始化是指定容器长度16

  - 某个node节点长度大于等于8

    > 如果容器长度小于 64，直接扩容
    >
    > 如果容器长度大于等于64，将链表转成红黑树
    >
    > 在进行扩容时，如果碰到节点为红黑树且长度小于等于6，就转成链表

  - 当数据数量达到阈值(容器长度 * 负载因子)

## 为什么不使用AVL树而使用红黑树

* 红黑树和AVL树都是**最常用的平衡二叉搜索树**，它们的查找、删除、修改都是O(lgn) time
* 但是在插入/删除操作时AVL树可能需要O（log n）旋转，而红黑树将需要最多两次旋转使其达到平衡

## 为什么阈值就是8和6呢

* 当hashCode离散性很好的时候，几乎不会有链表长度达到8

* 自定义key有可能使用了不好的hash算法，生成的hashCode离散性不好容易冲突

* 中间的7是作为缓冲，避免频繁转换

* 网上主流的答案：

  > 红黑树的平均查找长度是log(n)，如果长度为8，平均查找长度为log(8)=3，链表的平均查找长度为n/2，当长度为8时，平均查找长度为8/2=4，红黑树的查找效率更高，这才有转换成树的必要；
  > 链表长度如果是小于等于6，6/2=3，而log(6)=2.6，虽然速度也很快的，但是转化为树结构和生成树的时间并不会太短

* 另一解释(据说来自设计者)

  > 当hashCode离散性很好的时候，树型bin用到的概率非常小，因为数据均匀分布在每个bin中，几乎不会有bin中链表长度会达到阈值（树华门槛）。但是在随机hashCode下，离散性可能会变差，然而JDK又不能阻止用户实现这种不好的hash算法，因此就可能导致不均匀的数据分布。不过理想情况下随机hashCode算法下所有bin中节点的分布频率会遵循泊松分布，我们可以看到，一个bin中链表长度达到8个元素的概率为0.00000006，几乎是不可能事件。所以，之所以选择8，不是拍拍屁股决定的，而是根据概率统计决定的。由此可见，发展30年的Java每一项改动和优化都是非常严谨和科学的。